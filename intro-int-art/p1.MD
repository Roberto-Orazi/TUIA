- debil y fuerte
- simbolica y conexionista(subsimbolica)
- Agentes inteligentes y medio ambiente
<<<<<<< HEAD
- Machine Lerning
El aprendizaje supervisado es un subcampo de la inteligencia artificial, se centra en el desarrollo de algoritmos y modelos.
Con esos modelos las computadoras aprenden patrones a partir de datos sin ser explicitamente programada.
El objetivo es que el sistema realice tareas sin intervencion humana.
Mejorando su rendimiento con la experiencia.
Caracteristicas:
1. Aprende a partir de los datos: Utilizan datos para entrenarse y mejorar su rendimiento.
2. Generalizacion: Son capaces de aplicar lo que aprenden de los nuevos datos.
3. Adaptabilidad: Se adaptan y mejoran el rendimiento con la creciente de datos.
4. No siguen la programacion explicita: En vez de seguir las instrucciones, aprenden patrones y reglas a partir de los datos.

Modelos de aprendizaje automatico
1. Aprendizaje supervisado: El modelo se entrena con un conjunto de datos con ejemplos etiquetados, osea entrada y salida deseada, la idea es que luego haga predicciones o clasificaciones de los datos nuevos.
=======
- Machine Lerning El aprendizaje supervisado es un subcampo de la inteligencia artificial, se centra en el desarrollo de
algoritmos y modelos. Con esos modelos las computadoras aprenden patrones a partir de datos sin ser explicitamente
programada. El objetivo es que el sistema realice tareas sin intervencion humana. Mejorando su rendimiento con la
experiencia. Caracteristicas:
1. Aprende a partir de los datos: Utilizan datos para entrenarse y mejorar su rendimiento.
2. Generalizacion: Son capaces de aplicar lo que aprenden de los nuevos datos.
3. Adaptabilidad: Se adaptan y mejoran el rendimiento con la creciente de datos.
4. No siguen la programacion explicita: En vez de seguir las instrucciones, aprenden patrones y reglas a partir de los
   datos.

Modelos de aprendizaje automatico
1. Aprendizaje supervisado: El modelo se entrena con un conjunto de datos con ejemplos etiquetados, osea entrada y
   salida deseada, la idea es que luego haga predicciones o clasificaciones de los datos nuevos.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c

Clasificaciones:
- Logistic regression
- Decision tree
- Random forest
- Support vector machines

Algoritmos de regresion:
- Linear regression
- Regression trees
- Non Linear regression
- Bayesian Linear regression
- Polinomial regressionk


<<<<<<< HEAD
2. Aprendizaje no supervisado: El modelo se entrena con datos no etiquetados y debe encontrar patrones o relaciones entre ellos. Los ejemplos incluyen la agrupacion(Clustering) y la reduccion de dimensionalidad.
El clustering se usa para agrupar datos existentes de los que desconocemos sus caracteristicas en comun o quremos descubrir.
=======
2. Aprendizaje no supervisado: El modelo se entrena con datos no etiquetados y debe encontrar patrones o relaciones
entre ellos. Los ejemplos incluyen la agrupacion(Clustering) y la reduccion de dimensionalidad. El clustering se usa
para agrupar datos existentes de los que desconocemos sus caracteristicas en comun o quremos descubrir.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
- Deteccion de anomalias detecta elementos o eventos que no se ajustan al patron
- Reduccion de dimensiones, Se usa para evitar sobreajustes.

3. Aprendizaje por refuerzo: Interactua con el entorno y aprende mediante recompensas o penalizaciones.

<<<<<<< HEAD
4. Aprendizaje profundo, tambien llamado deep learning: Utiliza redes neuronales artificiales con multiples capas. Para aprender representaciones jerarquicas de datos.

Identificar modelos de underfitting y overfitting
Underfitting(Subajuste)
El modelo no es capas de identificar patrones, por lo tanto siempre tiene malos resultados.
Como prevenir underfitting:
=======
4. Aprendizaje profundo, tambien llamado deep learning: Utiliza redes neuronales artificiales con multiples capas. Para
   aprender representaciones jerarquicas de datos.

Identificar modelos de underfitting y overfitting Underfitting(Subajuste) El modelo no es capas de identificar patrones,
por lo tanto siempre tiene malos resultados. Como prevenir underfitting:
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
- Eliminar variables innecesarias
- usando modelos mas complejos
- Ajustando parametros
- Aumentando las iteraciones en los algoritmos iterativos


<<<<<<< HEAD
Overfitting(Sobreajuste)
El modelo esta sobreajustado cuando lo entrenamos con muchos datos.
El problema de tener tantos datos es que el modelo aprende del ruido o de datos inexactos.
Por eso es crucial equilibriar la cantidad y calidad de datos. Para evitar que aprenda de ruido y garantizar una generalizacion efectiva a nuevos datos.
=======
Overfitting(Sobreajuste) El modelo esta sobreajustado cuando lo entrenamos con muchos datos. El problema de tener tantos
datos es que el modelo aprende del ruido o de datos inexactos. Por eso es crucial equilibriar la cantidad y calidad de
datos. Para evitar que aprenda de ruido y garantizar una generalizacion efectiva a nuevos datos.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c

Como prevenir el overfitting:
- Dividir datos en: Training, validacion y testing
- Ajustando los parametros
- Usar modelos mas simples
- Datos que vienen de distintas distribuciones
- Bajando numero de iteraciones en los algoritmos iterativos

<<<<<<< HEAD
Varianza
La varianza es cuanto cambia el modelo si lo ajustamos a otros datos de entrenamiento. Si entrenamos un modelo muy especifico no nos va a servir para otros datos distintos.

Sesgo
El sesgo es la aproximacion del modelo con la realidad. Los modelos flexibles tienen bajo sesgo.
=======
Varianza La varianza es cuanto cambia el modelo si lo ajustamos a otros datos de entrenamiento. Si entrenamos un modelo
muy especifico no nos va a servir para otros datos distintos.

Sesgo El sesgo es la aproximacion del modelo con la realidad. Los modelos flexibles tienen bajo sesgo.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c


Balance Sesgo / Varianza

<<<<<<< HEAD
El overfitting tiene alta varianza
El underfitting tiene alto sesgo
El good balance tiene bajo sesgo y baja varianza
=======
El overfitting tiene alta varianza El underfitting tiene alto sesgo El good balance tiene bajo sesgo y baja varianza
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c

## Business Intelligence
Es el uso de datos para obtener informacion valiosa y tomar decisiones empresariales informadas.

Tecnologicas que forman parte del BI
- Delivery
- Reporting
- Permormance management
- Supporting applications
- Analitics
- Discovery & integration
- Data
- Infrastructure

## Data analitycs
<<<<<<< HEAD
Le permite a las empresas y organizaciones analizar los datos para generar conocimiento.
Con esas herramientas analticas, inforamn, diagnostican, predicen y automatizan decisiones en multiples areas del negocio.


Jerarquia DIKW(Data, Information, Knowledge, Wisdom)
Sabiduria:
Capacidad de pensar entender, asimialr y elaborar informacion y utilizarla para resolver problemas.
Aplicacion etica del conocimiento en decisiones.

Conocimiento:
Complejidad de las experiencias para adquirirlo.
Informacion con significado y relaciones

Informacion:
Conjunto organizado de datos procesados
Datos organizados y contextualizados.

Dato:
Representacion simbolica de un atributo o variable.
Son hechos o cifras sin procesar

Las 4 herramientas de data analitics en toma de decisiones organizativas.
1. Descriptivo: Las herramientas proporcionan informacion sobre variables pasadas ofreciendo un panorama cuantitativo y de estado
2. Diagnostico: Permite analizar porque ocurrieron ciertos eventos al contrastar tendencias pasadas y correlaciones entre variables, utilizando algoritmos matematicos de regresion. En otras palabras es definicion de diagnostico.
3. Predictivo: Se basa en datos y contextos pasados para crear escenarios futuros, respondiendo preguntas de neogico como proyecciones de venta.
4. Prescriptivo: Las decisiones se respaldan con analisis complejos que no solo preciden futuros estados, sino que tambien sugieren las mejores opciones de accion. Se analizan las consecuencias esperables segun las decisiones posibles.

Perfiles de data analytics
1. Analista de datos
Explica los datos de forma faicl de entender y resumida.
Usa los datos para tomar mejores decisiones.
Habilidades:
   - Estadistica
   - Comunicacion
   - Conocimiento del negocio
Herramientas:
=======
Le permite a las empresas y organizaciones analizar los datos para generar conocimiento. Con esas herramientas
analticas, inforamn, diagnostican, predicen y automatizan decisiones en multiples areas del negocio.


Jerarquia DIKW(Data, Information, Knowledge, Wisdom) Sabiduria: Capacidad de pensar entender, asimialr y elaborar
informacion y utilizarla para resolver problemas. Aplicacion etica del conocimiento en decisiones.

Conocimiento: Complejidad de las experiencias para adquirirlo. Informacion con significado y relaciones

Informacion: Conjunto organizado de datos procesados Datos organizados y contextualizados.

Dato: Representacion simbolica de un atributo o variable. Son hechos o cifras sin procesar

Las 4 herramientas de data analitics en toma de decisiones organizativas.
1. Descriptivo: Las herramientas proporcionan informacion sobre variables pasadas ofreciendo un panorama cuantitativo y
   de estado
2. Diagnostico: Permite analizar porque ocurrieron ciertos eventos al contrastar tendencias pasadas y correlaciones
   entre variables, utilizando algoritmos matematicos de regresion. En otras palabras es definicion de diagnostico.
3. Predictivo: Se basa en datos y contextos pasados para crear escenarios futuros, respondiendo preguntas de neogico
   como proyecciones de venta.
4. Prescriptivo: Las decisiones se respaldan con analisis complejos que no solo preciden futuros estados, sino que
   tambien sugieren las mejores opciones de accion. Se analizan las consecuencias esperables segun las decisiones
   posibles.

Perfiles de data analytics
1. Analista de datos Explica los datos de forma faicl de entender y resumida. Usa los datos para tomar mejores
decisiones. Habilidades:
   - Estadistica
   - Comunicacion
   - Conocimiento del negocio Herramientas:
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
   - Excel
   - Power BI
   - SQL

<<<<<<< HEAD
2. Ingeniero de datos
Prepara los datos para el analisis.
El objetivo es desarrollar y mantener las estructuras
de datos necesarios para obtener la informacion
Habilidades:
   - Matematicas
   - Programacion
   - Big Data
Herramientas:
   - Python
   - SQL

3. Cientifico de datos
Utiliza algoritmos de aprendizaje automatico para identificar
la ocurrencia en un evento particular futuro.
Habilidades:
   - Matematicas
   - Programacion
   - Big Data
Herramientas:
   - Python
   - SQL

Big Data
Se refiere a los conjuntos de datos extremadamente grandes y complejos que superan la capacidad e las herramientas tradicionales de procesamiento y analisis de datos.
Se les conoce como las 5V:
=======
2. Ingeniero de datos Prepara los datos para el analisis. El objetivo es desarrollar y mantener las estructuras de datos
necesarios para obtener la informacion Habilidades:
   - Matematicas
   - Programacion
   - Big Data Herramientas:
   - Python
   - SQL

3. Cientifico de datos Utiliza algoritmos de aprendizaje automatico para identificar la ocurrencia en un evento
particular futuro. Habilidades:
   - Matematicas
   - Programacion
   - Big Data Herramientas:
   - Python
   - SQL

Big Data Se refiere a los conjuntos de datos extremadamente grandes y complejos que superan la capacidad e las
herramientas tradicionales de procesamiento y analisis de datos. Se les conoce como las 5V:
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
   - Volumen: Cantidad masiva de datos
   - Velocidad: Rapidez con la que se generan o procesan los datos
   - Variedad: Diversidad en los tipos de datos, ya sean estructuras o no estructurados.
   - Valor: Capacidad de convertir datos en informacion valiosa.
   - Veracidad: La precision y confiabilidad de los datos

<<<<<<< HEAD
Base de datos
Es un conjunto organizado de datos que se almacenan en un sistema de gestion de bd

Base de datos relacionales SQL
Sistemas de gestión de bases de datos (SGBD) que organizan y administran datos de manera estructurada.
ACID:
Atomicidad: Transacciones completas o no se realizan
Consistencia: Mantienen la integridad de los datos
Aislamiento: Las transacciones son independientes
Durabilidad: Los cambios son permanentes incluso tras fallos

Base de datos no relacionales NOSQL
Alternativa a las bases de datos relacionales, ofrecen flexibilidad, escalabilidad y alto rendimiento.
BASE:
Basicamente disponible(Basically Available): Garantiza disponibilidad incluso en fallos parciales.
Suave: Permite que los datos cambien con el tiempo, y no estan consistentes todo el tiempo
Tolerancia a fallos(Eventually Consistent): Aunque no son consistentes en tiempo real, al tiempo si son consistentes


Repositorio de datos
Es un lugar centralizado para almacenar y gestionar datos, Facilita el acceso, control y busqueda eficiente.
=======
Base de datos Es un conjunto organizado de datos que se almacenan en un sistema de gestion de bd

Base de datos relacionales SQL Sistemas de gestión de bases de datos (SGBD) que organizan y administran datos de manera
estructurada. ACID: Atomicidad: Transacciones completas o no se realizan Consistencia: Mantienen la integridad de los
datos Aislamiento: Las transacciones son independientes Durabilidad: Los cambios son permanentes incluso tras fallos

Base de datos no relacionales NOSQL Alternativa a las bases de datos relacionales, ofrecen flexibilidad, escalabilidad y
alto rendimiento. BASE: Basicamente disponible(Basically Available): Garantiza disponibilidad incluso en fallos
parciales. Suave: Permite que los datos cambien con el tiempo, y no estan consistentes todo el tiempo Tolerancia a
fallos(Eventually Consistent): Aunque no son consistentes en tiempo real, al tiempo si son consistentes


Repositorio de datos Es un lugar centralizado para almacenar y gestionar datos, Facilita el acceso, control y busqueda
eficiente.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c

1. Data lake:
    - Almacena datos crudos y no estructurados
    - Proporciona flexibilidad
    - Multiples fuentes de origen


2. Data Warehouse:
    - Almacena datos procesados y estructurados
    - Centralizado para respaldar informes analisis y toma de decisiones empresariales.


3. Data Mart:
- Version mas pequeña y especializada de DW
- Se usa para satisfacer necesidades especificas de un grupo de usuarios o dpto


<<<<<<< HEAD
El ETL es el proceso de extraccion, transformacion y carga que conecta el DL con el DW organizando datos de diversas fuentes para su almacenamiento y su analisis estructurado.


E: Recopila los datos de diversas fuentes y los prepara para el procesamiento
T: Someten los datos a un proceso de limpieza y agregacion para que sean mas coherentes para el analisis
L: Finalmente los datos se cargan en un repositorio(DW) para su almacenamiento y analisis posterior.

Data Minning
La mineria de datos, es extraer informacion valiosa de los datos y organizarla de manera comprensible para uso posterior.
Para ello usa modelos de IA, estadistica y DB.

Data visualization
Es la visulizacion de datos mediante graficos, diagramas, mapas u otra representacion visual.

## Deep learning

Perceptron
Es un programa que toma datos de entrada, las Multiplica por un peso, y realiza una suma ponderada en la que cada entrada participa dependiendo del peso asignado.

entrada pesos Suma Funcion de activacion Salida
x1   .   w1 -> ∑ -> φ -> Y
x2   .   w2 -> ∑ -> φ -> Y

La sumatoria es de x1,x2,xn con los pesos de las mismas para llegar al resultado
Tener en cuenta que existe un b que es un termino independiente que varia para lograr el resultado
W.X + b <= 0 -> Y = 0
W.X + b > 0 -> Y = 1

Limitaciones
1. Linealidad:
El problema de la linealidad viene que los perceptrones solo modelan funciones lineales. No pueden capturar patrones o relaciones no lineales. Para ello se requieren modelos mas complejos o redes neuronales multicapa
2. XOR: Es el or estricto por lo tanto tiene
0 1
1 0
Y una linea no puede dividir

Funciones de activacion
Las funciones de activacion en redes neuronales son escenciales para decidir si una neurona debe activarse y transmitir su señal.

Perceptron multicapa(MLP)
Es una red neuronal con multiples capas que supera la limtacion de separacion lineal del perceptron. 
Tiene 3 capas
=======
El ETL es el proceso de extraccion, transformacion y carga que conecta el DL con el DW organizando datos de diversas
fuentes para su almacenamiento y su analisis estructurado.


E: Recopila los datos de diversas fuentes y los prepara para el procesamiento T: Someten los datos a un proceso de
limpieza y agregacion para que sean mas coherentes para el analisis L: Finalmente los datos se cargan en un
repositorio(DW) para su almacenamiento y analisis posterior.

Data Minning La mineria de datos, es extraer informacion valiosa de los datos y organizarla de manera comprensible para
uso posterior. Para ello usa modelos de IA, estadistica y DB.

Data visualization Es la visulizacion de datos mediante graficos, diagramas, mapas u otra representacion visual.

## Deep learning

Perceptron Es un programa que toma datos de entrada, las Multiplica por un peso, y realiza una suma ponderada en la que
cada entrada participa dependiendo del peso asignado.

entrada pesos Suma Funcion de activacion Salida x1   .   w1 -> ∑ -> φ -> Y x2   .   w2 -> ∑ -> φ -> Y

La sumatoria es de x1,x2,xn con los pesos de las mismas para llegar al resultado Tener en cuenta que existe un b que es
un termino independiente que varia para lograr el resultado W.X + b <= 0 -> Y = 0 W.X + b > 0 -> Y = 1

Limitaciones
1. Linealidad: El problema de la linealidad viene que los perceptrones solo modelan funciones lineales. No pueden
capturar patrones o relaciones no lineales. Para ello se requieren modelos mas complejos o redes neuronales multicapa
2. XOR: Es el or estricto por lo tanto tiene 0 1 1 0 Y una linea no puede dividir

Funciones de activacion Las funciones de activacion en redes neuronales son escenciales para decidir si una neurona debe
activarse y transmitir su señal.

Perceptron multicapa(MLP) Es una red neuronal con multiples capas que supera la limtacion de separacion lineal del
perceptron. Tiene 3 capas
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
1. Capas de Entradas:
- Recibe los datos de entrada
- Transmite la informacion atraves de conexiones a la capa oculta
2. Capas Ocultas:
- Son capas intermedias que aplican funciones de activacion no lineales
- Realizan calculos intermedios
- Permiten a la red capturar y modelar relaciones no lineales en los datos
3. Capas de salida:
- Produce la salida final de la red
- Refleja la naturaleza del problema a resolver(Clasificacion, regresion u otra tarea)



<<<<<<< HEAD
BackPropagation
Se ajustan pesos y sesgos para minimizar el error entre las salidas previstas y reales. Esto se hace mediante entrenamiento de aprendizaje supervisado.
En este proceso:
=======
BackPropagation Se ajustan pesos y sesgos para minimizar el error entre las salidas previstas y reales. Esto se hace
mediante entrenamiento de aprendizaje supervisado. En este proceso:
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c
1. Comparacion de resultados, compara la salida estimada con la real, obteniendo un error
2. Identificacion de culpables, Busca la contribucion de cada peso a la diferencia mediante el gradiente
3. Ajuste de pesos, ajusta los pesos identificados para reducir el error


<<<<<<< HEAD
Features
Son las caracteristicas/cualidades del objeto, y describen/representan los datos. Se usan como datos de entrara para algoritmos de aprendizaje automatico. Gracias a estas caracteristicas permiten al algoritmo analizar, predecir o clasificar.


DCNN Deep convultional neural networks
Son redes neuronales profundas diseñadas para tareas de vision por computadora

Las Capas de convolucion aplican filtros para detectar patrones visuales en regiones locales de la imagen,
La jerarquia de features, se emplean multiples capas de convolucion para adquirir features visuales cada vezz mas complejas o abtractas a medida que se profundiza en la red.

Deteccion de features
Es el proceso de identificar y localizar features relevantes en imagenes o datos de imagenes.

Kernels
Son matrices o filtros utilizadas en las capas de convolucion.
Se desplazan por la imagen realizando multiplicaciones ponderadas para crear mapas de features.
=======
Features Son las caracteristicas/cualidades del objeto, y describen/representan los datos. Se usan como datos de entrara
para algoritmos de aprendizaje automatico. Gracias a estas caracteristicas permiten al algoritmo analizar, predecir o
clasificar.


DCNN Deep convultional neural networks Son redes neuronales profundas diseñadas para tareas de vision por computadora

Las Capas de convolucion aplican filtros para detectar patrones visuales en regiones locales de la imagen, La jerarquia
de features, se emplean multiples capas de convolucion para adquirir features visuales cada vezz mas complejas o
abtractas a medida que se profundiza en la red.

Deteccion de features Es el proceso de identificar y localizar features relevantes en imagenes o datos de imagenes.

Kernels Son matrices o filtros utilizadas en las capas de convolucion. Se desplazan por la imagen realizando
multiplicaciones ponderadas para crear mapas de features.
>>>>>>> b3598058efb03adfd8e098a3ca703ab092722d8c

